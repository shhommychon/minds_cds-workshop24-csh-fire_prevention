{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l5oj4EB7Z3ov",
        "Qk1NnUnxbewL",
        "-uNYbM_mpZ_2",
        "BJmq0bFUsGIH",
        "yOt5vxmUpZ_3",
        "j2krgjsUzqBc",
        "ARbBvTz40vID"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shhommychon/minds_cds-workshop24-csh-fire_prevention/blob/feature%2Freplit_demo1/replit_demo1/notebook.ipynb)"
      ],
      "metadata": {
        "id": "TPRBkPt1YbFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo 1: Whisper & ChatGPT API"
      ],
      "metadata": {
        "id": "Bnx5XXdzbTEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### setup code"
      ],
      "metadata": {
        "id": "l5oj4EB7Z3ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install httpx==0.27.2  # updated `openai` library causes dependency issues when higher version of `httpx` is used\n",
        "!pip install pydub"
      ],
      "metadata": {
        "id": "YUwVQLFrYa5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"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\")"
      ],
      "metadata": {
        "id": "tirNs7CMbuIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper API"
      ],
      "metadata": {
        "id": "G0XYWafqbkm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = \"/path/to/your/file\"  # TODO: upload your audio file and enter the path here (Note that most of the extensions will be automatically converted to desired extension)"
      ],
      "metadata": {
        "id": "85k0v5ribmfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### misc"
      ],
      "metadata": {
        "id": "Qk1NnUnxbewL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vN1BhL4XjF2"
      },
      "outputs": [],
      "source": [
        "# audio util\n",
        "\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "_supported_extensions = (\n",
        "    \".mp3\",\n",
        "    \".aac\", \".m4a\", \".wav\", \".flac\", \".aiff\",\n",
        "    \".ogg\", \".mp4\", \".mpeg\", \".mpga\", \".webm\",\n",
        ")\n",
        "\n",
        "\n",
        "def convert_to_mp3(file_path):\n",
        "    \"\"\"Convert audio file to MP3 format\"\"\"\n",
        "\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension not in _supported_extensions:\n",
        "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "    if file_extension == \".mp3\":\n",
        "        return file_path\n",
        "\n",
        "    output_path = os.path.splitext(file_path)[0] + \"_converted.mp3\"\n",
        "\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(file_path)\n",
        "        audio.export(output_path, format=\"mp3\")\n",
        "\n",
        "        # Check file size\n",
        "        if os.path.getsize(output_path) > 25 * 1024 * 1024:  # 25MB in bytes\n",
        "            os.remove(output_path)\n",
        "            raise ValueError(\"Converted file exceeds 25MB limit\")\n",
        "\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        if os.path.exists(output_path):\n",
        "            os.remove(output_path)\n",
        "        raise Exception(f\"Error converting audio: {str(e)}\")\n",
        "\n",
        "audio_file_path = convert_to_mp3(audio_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription (basic usage)"
      ],
      "metadata": {
        "id": "nM8wjTFpbNBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file= open(audio_file_path, \"rb\")\n",
        "transcription = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=audio_file,\n",
        ")"
      ],
      "metadata": {
        "id": "wSF-jiVf767Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription.text)"
      ],
      "metadata": {
        "id": "0q6mPN5Rgt-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription.language)"
      ],
      "metadata": {
        "id": "tqhf6o3tguVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription.duration)"
      ],
      "metadata": {
        "id": "o69xtV-8gup3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription (specific response format)"
      ],
      "metadata": {
        "id": "5LiwcB0QeVqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = open(audio_file_path, \"rb\")\n",
        "transcription = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=audio_file,\n",
        "    response_format=\"text\",     # use one of these options: json, text, srt, verbose_json, or vtt.\n",
        ")"
      ],
      "metadata": {
        "id": "Kmj0WUtvhS4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcription.text)"
      ],
      "metadata": {
        "id": "kH6AC4dpeVq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription with Timestamps"
      ],
      "metadata": {
        "id": "LkLkaMZmo3zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file = open(audio_file_path, \"rb\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "    file=audio_file,\n",
        "    model=\"whisper-1\",\n",
        "    response_format=\"verbose_json\",\n",
        "    timestamp_granularities=[\"word\"],   # use one of these options: [\"word\"], [\"segments\"], [\"word\", \"segments\"].\n",
        ")"
      ],
      "metadata": {
        "id": "YEKBPZYIgUKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcript.words)"
      ],
      "metadata": {
        "id": "lxrAlWcNpJm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transcript.segments)"
      ],
      "metadata": {
        "id": "VSYwLdeApKCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Bonus 1] Whisper via `huggingface` (open source)"
      ],
      "metadata": {
        "id": "-uNYbM_mpZ_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### setup code\n",
        "- may require restarting server"
      ],
      "metadata": {
        "id": "BJmq0bFUsGIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers datasets[audio] accelerate"
      ],
      "metadata": {
        "id": "vk6DcdHQsLQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples of using open source models"
      ],
      "metadata": {
        "id": "ZNS_nJCqsFzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = \"/path/to/your/file\"  # TODO: upload your audio file and enter the path here (Note that most of the extensions will be automatically converted to desired extension)"
      ],
      "metadata": {
        "id": "1j06vpHhpZ_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For full list of available model cards, visit huggingface.co\n",
        "model_cards = {\n",
        "    \"official\": [\n",
        "        \"openai/whisper-tiny\",      #   39M param (at least  156MB GPU memory need for inference)\n",
        "        \"openai/whisper-base\",      #   74M param (at least  296MB GPU memory need for inference)\n",
        "        \"openai/whisper-small\",     #  244M param (at least  976MB GPU memory need for inference)\n",
        "        \"openai/whisper-medium\",    #  769M param (at least 3076MB GPU memory need for inference)\n",
        "        \"openai/whisper-large\",     # 1550M param (at least 6200MB GPU memory need for inference)\n",
        "        \"openai/whisper-large-v2\",  # 1550M param (at least 6200MB GPU memory need for inference)\n",
        "        \"openai/whisper-large-v3\",  # 1550M param (at least 6200MB GPU memory need for inference)\n",
        "    ],\n",
        "    \"custom\": [\n",
        "        \"circulus/whisper-ko-adult-v1\",\n",
        "        \"circulus/whisper-ko-senior-v1\",\n",
        "        \"seastar105/whisper-small-ko-zeroth\",\n",
        "        \"seastar105/whisper-medium-ko-zeroth\",\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "JE0gTwVqpxgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_model_card = model_cards[\"official\"][3]  # choose your model"
      ],
      "metadata": {
        "id": "gefxCAvtqwoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### misc"
      ],
      "metadata": {
        "id": "yOt5vxmUpZ_3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpPYu_9UpZ_3"
      },
      "outputs": [],
      "source": [
        "# audio util\n",
        "\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "_supported_extensions = (\n",
        "    \".mp3\",\n",
        "    \".aac\", \".m4a\", \".wav\", \".flac\", \".aiff\",\n",
        "    \".ogg\", \".mp4\", \".mpeg\", \".mpga\", \".webm\",\n",
        ")\n",
        "\n",
        "\n",
        "def convert_to_mp3(file_path):\n",
        "    \"\"\"Convert audio file to MP3 format\"\"\"\n",
        "\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension not in _supported_extensions:\n",
        "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
        "\n",
        "    if file_extension == \".mp3\":\n",
        "        return file_path\n",
        "\n",
        "    output_path = os.path.splitext(file_path)[0] + \"_converted.mp3\"\n",
        "\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(file_path)\n",
        "        audio.export(output_path, format=\"mp3\")\n",
        "\n",
        "        # Check file size\n",
        "        if os.path.getsize(output_path) > 25 * 1024 * 1024:  # 25MB in bytes\n",
        "            os.remove(output_path)\n",
        "            raise ValueError(\"Converted file exceeds 25MB limit\")\n",
        "\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        if os.path.exists(output_path):\n",
        "            os.remove(output_path)\n",
        "        raise Exception(f\"Error converting audio: {str(e)}\")\n",
        "\n",
        "audio_file_path = convert_to_mp3(audio_file_path)\n",
        "\n",
        "\n",
        "# If you face an error about having passed more than certain number of mel input\n",
        "# features, it means that your audio file is too long. Consider using slice of\n",
        "# your audio file.\n",
        "\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def cut_audio(file_path, start_time, end_time):\n",
        "    \"\"\"Cuts an audio file from start_time to end_time and saves it as a new file.\"\"\"\n",
        "    try:\n",
        "        audio = AudioSegment.from_file(file_path)\n",
        "        cut_audio = audio[start_time * 1000:end_time * 1000]  # Convert seconds to milliseconds\n",
        "\n",
        "        output_path = os.path.splitext(file_path)[0] + \"_sliced.mp3\"\n",
        "\n",
        "        cut_audio.export(output_path, format=\"mp3\")\n",
        "        return output_path\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file '{file_path}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# # Example usage:\n",
        "# audio_file_path = cut_audio(audio_file_path, 0, 8)  # cut the first 8 seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface utils\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "def huggingface_transcribe(model:str, file_path:str, **kwargs):\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    whisper_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        model, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        "    )\n",
        "    whisper_model.to(device)\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model)\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=whisper_model,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        feature_extractor=processor.feature_extractor,\n",
        "        torch_dtype=torch_dtype,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    return pipe(file_path)\n"
      ],
      "metadata": {
        "id": "YI7F2IGIh0Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Basic usage"
      ],
      "metadata": {
        "id": "H9sj1yXjvMZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = huggingface_transcribe(\n",
        "    model=chosen_model_card,\n",
        "    file_path=audio_file_path,\n",
        ")\n",
        "print(transcription[\"text\"])"
      ],
      "metadata": {
        "id": "PDE5mRq7u8m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Advanced usage of using hidden layers"
      ],
      "metadata": {
        "id": "FpjWdpXxvq6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import WhisperModel, WhisperProcessor\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "import io\n",
        "\n",
        "def get_whisper_embeddings(model:str, data:np.ndarray, layer_index=-1):\n",
        "    \"\"\"Extract hidden embeddings from a specific encoder layer of Whisper.\n",
        "\n",
        "    Args:\n",
        "        model (str): Model ID\n",
        "        data (np.ndarray): Audio array\n",
        "        layer_index (int): Which encoder layer to extract embeddings from (-1 for last layer)\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Hidden representations from specified layer\n",
        "    \"\"\"\n",
        "    # Load model and processor\n",
        "    wmodel = WhisperModel.from_pretrained(model)\n",
        "    wprocessor = WhisperProcessor.from_pretrained(model)\n",
        "\n",
        "    # Load and process audio\n",
        "    audio_input = wprocessor(data, return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "    # Get encoder outputs with output_hidden_states=True\n",
        "    with torch.no_grad():\n",
        "        outputs = wmodel.encoder(\n",
        "            audio_input.input_features,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "    # Access hidden states\n",
        "    # hidden_states is tuple of tensors, one for each layer including input embeddings\n",
        "    hidden_states = outputs.hidden_states\n",
        "\n",
        "    # Get embeddings from specified layer\n",
        "    layer_embeddings = hidden_states[layer_index].squeeze(0).numpy()\n",
        "\n",
        "    return layer_embeddings\n",
        "\n",
        "\n",
        "audio = AudioSegment.from_file(audio_file_path)\n",
        "audio_data = np.array(audio.get_array_of_samples())\n",
        "hidden_states = get_whisper_embeddings(\n",
        "    model=chosen_model_card,\n",
        "    data=audio_data,\n",
        "    layer_index=-1,\n",
        ")\n",
        "display(hidden_states)"
      ],
      "metadata": {
        "id": "4348EsDLksta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Bonus 2] CrisperWhisper (open source)\n",
        "- calculate fillers as well\n",
        "- [license](https://huggingface.co/nyrahealth/CrisperWhisper) agreement required"
      ],
      "metadata": {
        "id": "j2krgjsUzqBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "gVmmnk7S1V_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = \"/path/to/your/file\"  # TODO: upload your audio file and enter the path here (Note that most of the extensions will be automatically converted to desired extension)"
      ],
      "metadata": {
        "id": "M1r4g5jq4hfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_model_card = \"nyrahealth/CrisperWhisper\""
      ],
      "metadata": {
        "id": "DMMqtrUj4rRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### setup code"
      ],
      "metadata": {
        "id": "_rIBkoIl0tI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nyrahealth/CrisperWhisper.git\n",
        "import sys; sys.path = [\"/content/CrisperWhisper\"] + sys.path"
      ],
      "metadata": {
        "id": "GpVBwmr3zpYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### misc"
      ],
      "metadata": {
        "id": "ARbBvTz40vID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crisper utils\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from utils import adjust_pauses_for_hf_pipeline_output\n",
        "\n",
        "def crisper_transcribe(model:str, file_path:str, **kwargs):\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "    whisper_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "        model, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
        "    )\n",
        "    whisper_model.to(device)\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"automatic-speech-recognition\",\n",
        "        model=whisper_model,\n",
        "        tokenizer=processor.tokenizer,\n",
        "        feature_extractor=processor.feature_extractor,\n",
        "        chunk_length_s=30,\n",
        "        batch_size=16,\n",
        "        return_timestamps='word',\n",
        "        torch_dtype=torch_dtype,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    hf_pipeline_output = pipe(file_path)\n",
        "    return adjust_pauses_for_hf_pipeline_output(hf_pipeline_output)"
      ],
      "metadata": {
        "id": "-4zDRtdX4QmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Basic usage"
      ],
      "metadata": {
        "id": "YX_J8LBj5Jqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcription = crisper_transcribe(\n",
        "    model=chosen_model_card,\n",
        "    file_path=audio_file_path,\n",
        ")\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "4Lm4f7gM5LIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ChatGPT API"
      ],
      "metadata": {
        "id": "OwrUs4zD5jqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "\n",
        "\"\"\".strip()     # TODO: add your system prompt"
      ],
      "metadata": {
        "id": "QqJS4knR5jqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"\"\"\n",
        "\n",
        "\"\"\".strip()     # TODO: add your user prompt"
      ],
      "metadata": {
        "id": "n8mCxbhC6in2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt},\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=conversation_history\n",
        ")\n",
        "\n",
        "assistant_response = response.choices[0].message.content\n",
        "print(assistant_response)"
      ],
      "metadata": {
        "id": "U1OeRBNZ6QwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt2 = \"\"\"\n",
        "\n",
        "\"\"\".strip()     # TODO: add your next user prompt"
      ],
      "metadata": {
        "id": "u6KMb9FF6KqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history.extend([\n",
        "    {\"role\": \"assistant\", \"content\": assistant_response},\n",
        "    {\"role\": \"user\", \"content\": user_prompt2},\n",
        "])\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=conversation_history\n",
        ")\n",
        "\n",
        "assistant_response2 = response.choices[0].message.content\n",
        "print(assistant_response2)"
      ],
      "metadata": {
        "id": "K--tmEzN64Gl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}